{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from torchvision.models import efficientnet_v2_s\n",
    "from torch import Tensor\n",
    "from torchvision.ops.misc import Conv2dNormActivation, SqueezeExcitation    \n",
    "from typing import Callable, Optional, Tuple, List, Dict, Any, Sequence, Union\n",
    "import math\n",
    "import copy\n",
    "from torchvision.ops import StochasticDepth\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _make_divisible(v, divisor=8, min_value=None):\n",
    "    \"\"\"\n",
    "    This function takes an input value `v` and ensures that it is divisible by\n",
    "    the specified `divisor`. Optionally, a `min_value` can be provided to ensure\n",
    "    that the returned value is not less than a certain minimum value.\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class _MBConvConfig:\n",
    "    expand_ratio: float\n",
    "    kernel: int\n",
    "    stride: int\n",
    "    input_channels: int\n",
    "    out_channels: int\n",
    "    num_layers: int\n",
    "    block: Callable[..., nn.Module]\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_mult: float, min_value: Optional[int] = None) -> int:\n",
    "        return _make_divisible(channels * width_mult, 8, min_value)\n",
    "\n",
    "\n",
    "class MBConvConfig(_MBConvConfig):\n",
    "    # Stores information listed at Table 1 of the EfficientNet paper & Table 4 of the EfficientNetV2 paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        expand_ratio: float,\n",
    "        kernel: int,\n",
    "        stride: int,\n",
    "        input_channels: int,\n",
    "        out_channels: int,\n",
    "        num_layers: int,\n",
    "        width_mult: float = 1.0,\n",
    "        depth_mult: float = 1.0,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        num_layers = self.adjust_depth(num_layers, depth_mult)\n",
    "        if block is None:\n",
    "            block = MBConv\n",
    "        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_depth(num_layers: int, depth_mult: float):\n",
    "        return int(math.ceil(num_layers * depth_mult))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedMBConvConfig(_MBConvConfig):\n",
    "    # Stores information listed at Table 4 of the EfficientNetV2 paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        expand_ratio: float,\n",
    "        kernel: int,\n",
    "        stride: int,\n",
    "        input_channels: int,\n",
    "        out_channels: int,\n",
    "        num_layers: int,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        if block is None:\n",
    "            block = FusedMBConv\n",
    "        super().__init__(expand_ratio, kernel, stride, input_channels, out_channels, num_layers, block)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnf: MBConvConfig,\n",
    "        stochastic_depth_prob: float,\n",
    "        norm_layer: Callable[..., nn.Module],\n",
    "        se_layer: Callable[..., nn.Module] = SqueezeExcitation,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError(\"illegal stride value\")\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.SiLU\n",
    "\n",
    "        # expand\n",
    "        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n",
    "        if expanded_channels != cnf.input_channels:\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(\n",
    "                    cnf.input_channels,\n",
    "                    expanded_channels,\n",
    "                    kernel_size=1,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=activation_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # depthwise\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                expanded_channels,\n",
    "                expanded_channels,\n",
    "                kernel_size=cnf.kernel,\n",
    "                stride=cnf.stride,\n",
    "                groups=expanded_channels,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=activation_layer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # squeeze and excitation\n",
    "        squeeze_channels = max(1, cnf.input_channels // 4)\n",
    "        layers.append(se_layer(expanded_channels, squeeze_channels, activation=partial(nn.SiLU, inplace=True)))\n",
    "\n",
    "        # project\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.out_channels = cnf.out_channels\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result = self.stochastic_depth(result)\n",
    "            result += input\n",
    "        return result\n",
    "\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnf: FusedMBConvConfig,\n",
    "        stochastic_depth_prob: float,\n",
    "        norm_layer: Callable[..., nn.Module],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError(\"illegal stride value\")\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.SiLU\n",
    "\n",
    "        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n",
    "        if expanded_channels != cnf.input_channels:\n",
    "            # fused expand\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(\n",
    "                    cnf.input_channels,\n",
    "                    expanded_channels,\n",
    "                    kernel_size=cnf.kernel,\n",
    "                    stride=cnf.stride,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=activation_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # project\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(\n",
    "                    expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=None\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            layers.append(\n",
    "                Conv2dNormActivation(\n",
    "                    cnf.input_channels,\n",
    "                    cnf.out_channels,\n",
    "                    kernel_size=cnf.kernel,\n",
    "                    stride=cnf.stride,\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=activation_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.out_channels = cnf.out_channels\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result = self.stochastic_depth(result)\n",
    "            result += input\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n",
    "        stochastic_depth_prob: float = 0.2,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        last_channel: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        EfficientNet V1 and V2 main class\n",
    "\n",
    "        Args:\n",
    "            inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]]): Network structure\n",
    "            stochastic_depth_prob (float): The stochastic depth probability\n",
    "            num_classes (int): Number of classes\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not inverted_residual_setting:\n",
    "            raise ValueError(\"The inverted_residual_setting should not be empty\")\n",
    "        elif not (\n",
    "            isinstance(inverted_residual_setting, Sequence)\n",
    "            and all([isinstance(s, _MBConvConfig) for s in inverted_residual_setting])\n",
    "        ):\n",
    "            raise TypeError(\"The inverted_residual_setting should be List[MBConvConfig]\")\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        # building first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                in_channels, firstconv_output_channels, kernel_size=3, stride=1, norm_layer=norm_layer, activation_layer=nn.SiLU\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n",
    "        stage_block_id = 0\n",
    "        for cnf in inverted_residual_setting:\n",
    "            stage: List[nn.Module] = []\n",
    "            for _ in range(cnf.num_layers):\n",
    "                # copy to avoid modifications. shallow copy is enough\n",
    "                block_cnf = copy.copy(cnf)\n",
    "\n",
    "                # overwrite info if not the first conv in the stage\n",
    "                if stage:\n",
    "                    block_cnf.input_channels = block_cnf.out_channels\n",
    "                    block_cnf.stride = 1\n",
    "\n",
    "                # adjust stochastic depth probability based on the depth of the stage block\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / total_stage_blocks\n",
    "\n",
    "                stage.append(block_cnf.block(block_cnf, sd_prob, norm_layer))\n",
    "                stage_block_id += 1\n",
    "\n",
    "            layers.append(nn.Sequential(*stage))\n",
    "\n",
    "        # building last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = last_channel if last_channel is not None else 4 * lastconv_input_channels\n",
    "        layers.append(\n",
    "            Conv2dNormActivation(\n",
    "                lastconv_input_channels,\n",
    "                lastconv_output_channels,\n",
    "                kernel_size=1,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=nn.SiLU,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init_range = 1.0 / math.sqrt(m.out_features)\n",
    "                nn.init.uniform_(m.weight, -init_range, init_range)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _efficientnet(\n",
    "    inverted_residual_setting: Sequence[Union[MBConvConfig, FusedMBConvConfig]],\n",
    "    dropout: float,\n",
    "    last_channel: Optional[int],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> EfficientNet:\n",
    "\n",
    "    model = EfficientNet(inverted_residual_setting, dropout, last_channel=last_channel, **kwargs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand_ratio: float,\n",
    "# kernel: int,\n",
    "# stride: int,\n",
    "# input_channels: int,\n",
    "# out_channels: int,\n",
    "# num_layers: int,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FusedMBConvConfig'>\n"
     ]
    }
   ],
   "source": [
    "class_name = \"FusedMBConvConfig\"  # Example string representation of a class\n",
    "\n",
    "class_obj = globals().get(class_name)\n",
    "if class_obj:\n",
    "    print(class_obj)\n",
    "    # Output: <class '__main__.FusedMBConvConfig'>\n",
    "else:\n",
    "    print(f\"Class '{class_name}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_inverted_residual_setting(config_dict):\n",
    "    inverted_residual_setting = []\n",
    "    for config_key, config_value in config_dict.items():\n",
    "        config = globals().get(config_value['class_name'])\n",
    "        print(config)\n",
    "        if config:\n",
    "            args = config_value['args']\n",
    "            kwargs = config_value['kwargs']\n",
    "            inverted_residual_setting.append(config(*args, **kwargs))\n",
    "        else:\n",
    "            print(f\"Class '{config_value['class_name']}' not found.\")\n",
    "    return inverted_residual_setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand_ratio, kernel, stride, input_channels, out_channels, num_layers\n",
    "inverted_residual_setting = [\n",
    "    FusedMBConvConfig(1, 3, 1, 24, 24, 1),\n",
    "    FusedMBConvConfig(2, 3, 2, 24, 48, 1),\n",
    "    FusedMBConvConfig(2, 3, 1, 48, 96, 1),\n",
    "    FusedMBConvConfig(2, 3, 1, 96, 128, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(1, 1, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet(1, inverted_residual_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 50, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
      "      )\n",
      "      (1): FusedMBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Conv2dNormActivation(\n",
      "      (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "from ..base_model import BaseModel\n",
    "from ..modules import SetBlockWrapper, HorizontalPoolingPyramid, PackSequenceWrapper, SeparateFCs, SeparateBNNecks, conv1d, mlp_sigmoid, conv_bn, SetBlock, MSTE, ATA, SSFL, BasicConv1d, BasicConv2d, TemporalShift\n",
    "\n",
    "\n",
    "\n",
    "class Cell_Model(torch.nn.Module):\n",
    "    def __init__(self, inverted_residual_setting):\n",
    "        super(Cell_Model, self).__init__()\n",
    "        self.backbone = EfficientNet(3, inverted_residual_setting)\n",
    "        # backbone\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ipts, labs, _, _, seqL = inputs\n",
    "        sils = ipts[0] #n, c, s, h, w (in cstl paper: B x N x H x W, - batch, num of frames, height, width)\n",
    "        \n",
    "        sils = sils.permute(0, 2, 1, 3, 4).contiguous() #n, s, c, h, w\n",
    "        n, s, _, _, _ = sils.size()\n",
    "        del ipts\n",
    "        if len(sils.size()) == 4:\n",
    "            sils = sils.unsqueeze(2)\n",
    "        x = self.Backbone(sils) #n, s, c, h, w\n",
    "        if len(x.size()) == 4:\n",
    "            ns, c, h, w = x.size()\n",
    "            x = x.view(n, s, c, h, w)\n",
    "        x = x.max(-1)[0] + x.mean(-1) #Global Max pooling + Global Average Pooling\n",
    "        #n, s, c, k (K: as in paper: the number of horizontal division feature parts that correspond to body parts in some extent.)\n",
    "        t_f, t_s, t_l = self.multi_scale(x) #n, s, c, K\n",
    "        aggregated_feature = self.adaptive_aggregation(t_f, t_s, t_l) #K, n, c\n",
    "        part_classification, weighted_part_feature, selected_part_feature = self.salient_learning(t_f, t_s, t_l) #K, n, c\n",
    "        feature = torch.cat([aggregated_feature, weighted_part_feature, selected_part_feature], -1)\n",
    "        feature = feature.matmul(self.FCs) #n, c, p\n",
    "        feature = feature.permute(1, 2, 0).contiguous() #p, n, c\n",
    "        n, s, c, h, w = sils.size()\n",
    "        retval = {\n",
    "            'training_feat': {\n",
    "                'triplet': {'embeddings': feature, 'labels': labs},\n",
    "                'cstl_cross_entropy': {'part_prob': part_classification, 'label': labs}\n",
    "            },\n",
    "            'visual_summary': {\n",
    "                'image/sils': sils.view(n*s, c, h, w)\n",
    "            },\n",
    "            'inference_feat': {\n",
    "                'embeddings': feature\n",
    "            }\n",
    "        }\n",
    "        return retval\n",
    "        #return feature, part_classification.permute(1,0,2).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 30, 3, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 3, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(-1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
